{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F11vieLj7ECg",
        "outputId": "8f0d91d3-6990-4ac0-b728-b3c40c45525a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Text:\n",
            "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.'), ('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('member', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('member', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corp', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guest', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizen', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lose', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('call', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideal', 'NNS'), ('and', 'CC'), ('carry', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.'), ('Tonight', 'NN'), ('we', 'PRP'), ('be', 'VBP'), ('comfort', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('be', 'VBD'), ('take', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('be', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.'), ('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')'), ('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('react', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan.', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('.', '.'), ('White', 'NNP'), ('House', 'NNP'), ('photo', 'NN'), ('by', 'IN'), ('Eric', 'NNP'), ('DraperEvery', 'NNP'), ('time', 'NN'), ('I', 'PRP'), (\"'m\", 'VBP'), ('invited', 'JJ'), ('to', 'TO'), ('this', 'DT'), ('rostrum', 'NN'), (',', ','), ('I', 'PRP'), (\"'m\", 'VBP'), ('humble', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('privilege', 'NN'), (',', ','), ('and', 'CC'), ('mindful', 'NN'), ('of', 'IN'), ('the', 'DT'), ('history', 'NN'), ('we', 'PRP'), (\"'ve\", 'VBP'), ('see', 'VBN'), ('together', 'RB'), ('.', '.'), ('We', 'PRP'), ('have', 'VBP'), ('gather', 'VBN'), ('under', 'IN'), ('this', 'DT'), ('Capitol', 'NNP'), ('dome', 'NN'), ('in', 'IN'), ('moment', 'NNS'), ('of', 'IN'), ('national', 'JJ'), ('mourning', 'NN'), ('and', 'CC'), ('national', 'JJ'), ('achievement', 'NN'), ('.', '.'), ('We', 'PRP'), ('have', 'VBP'), ('serve', 'VBN'), ('America', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the required NLTK corpora and models\n",
        "nltk.download('state_union')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Get a sample text from the state_union corpus\n",
        "text = state_union.raw(\"2006-GWBush.txt\")[:1000]\n",
        "\n",
        "# Tokenize and tag the text\n",
        "tokenized = nltk.word_tokenize(text)\n",
        "tagged = nltk.pos_tag(tokenized)\n",
        "\n",
        "# Define a function to lemmatize words\n",
        "def lemmatize_words(word, pos):\n",
        "    if pos.startswith('N'):\n",
        "        return WordNetLemmatizer().lemmatize(word, pos='n')\n",
        "    elif pos.startswith('V'):\n",
        "        return WordNetLemmatizer().lemmatize(word, pos='v')\n",
        "    elif pos.startswith('J'):\n",
        "        return WordNetLemmatizer().lemmatize(word, pos='a')\n",
        "    elif pos.startswith('R'):\n",
        "        return WordNetLemmatizer().lemmatize(word, pos='r')\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "# Lemmatize the tagged text\n",
        "lemmatized = [(lemmatize_words(word, pos), pos) for word, pos in tagged]\n",
        "\n",
        "# Print the lemmatized text\n",
        "print(\"Lemmatized Text:\")\n",
        "print(lemmatized)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk import ne_chunk\n",
        "\n",
        "# Download the required NLTK corpora and models\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Define a function to extract named entities\n",
        "def extract_entities(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Tag the tokens with their part of speech\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    # Use the named entity chunker to extract named entities\n",
        "    chunked = ne_chunk(tagged)\n",
        "\n",
        "    # Initialize a list to hold the named entities\n",
        "    entities = []\n",
        "\n",
        "    # Iterate over the chunks and extract named entities\n",
        "    for chunk in chunked:\n",
        "        if hasattr(chunk, 'label') and chunk.label() in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
        "            entity = ' '.join(c[0] for c in chunk)\n",
        "            entity_type = chunk.label()\n",
        "            entities.append((entity, entity_type))\n",
        "\n",
        "    # Return the named entities\n",
        "    return entities\n",
        "\n",
        "# Test the function with a sample text\n",
        "text = \"Barack Obama was born in Hawaii and became the president of the United States. He was the leader of the Democratic Party.\"\n",
        "entities = extract_entities(text)\n",
        "print(entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJJCSBi57dKe",
        "outputId": "4b1c38f1-e23b-4e29-aebd-7e93565ad56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Barack', 'PERSON'), ('Obama', 'PERSON'), ('Democratic Party', 'ORGANIZATION')]\n"
          ]
        }
      ]
    }
  ]
}